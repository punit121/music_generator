{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f6aef36-2226-4e2b-8d34-ea7a7ff6cc63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training...\n",
      "Epoch 1/10, Loss: 2.0720\n",
      "Epoch 2/10, Loss: 1.9722\n",
      "Epoch 3/10, Loss: 1.2325\n",
      "Epoch 4/10, Loss: 0.4891\n",
      "Epoch 5/10, Loss: 0.2200\n",
      "Epoch 6/10, Loss: 0.1182\n",
      "Epoch 7/10, Loss: 0.0723\n",
      "Epoch 8/10, Loss: 0.0498\n",
      "Epoch 9/10, Loss: 0.0366\n",
      "Epoch 10/10, Loss: 0.0286\n",
      "Training Complete.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Device Configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define the LSTM-based RNN\n",
    "class MusicRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=2):\n",
    "        super(MusicRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        out, hidden = self.lstm(x, hidden)\n",
    "        out = self.fc(out)\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return (torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device),\n",
    "                torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device))\n",
    "\n",
    "\n",
    "# Preprocessing Data: Convert MIDI-like sequence to input-output pairs\n",
    "def preprocess_data(midi_data, sequence_length=50):\n",
    "    unique_notes = sorted(set(midi_data))\n",
    "    note_to_int = {note: i for i, note in enumerate(unique_notes)}\n",
    "    int_to_note = {i: note for i, note in enumerate(unique_notes)}\n",
    "\n",
    "    # Encode notes as integers\n",
    "    encoded_notes = [note_to_int[note] for note in midi_data]\n",
    "\n",
    "    # Create input-output sequences\n",
    "    input_sequences = []\n",
    "    output_sequences = []\n",
    "    for i in range(len(encoded_notes) - sequence_length):\n",
    "        input_sequences.append(encoded_notes[i:i + sequence_length])\n",
    "        output_sequences.append(encoded_notes[i + sequence_length])\n",
    "\n",
    "    return np.array(input_sequences), np.array(output_sequences), note_to_int, int_to_note\n",
    "\n",
    "\n",
    "# Custom Dataset for MIDI Sequences\n",
    "class MusicDataset(Dataset):\n",
    "    def __init__(self, inputs, outputs):\n",
    "        self.inputs = torch.tensor(inputs, dtype=torch.float32)\n",
    "        self.outputs = torch.tensor(outputs, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.outputs[idx]\n",
    "\n",
    "\n",
    "# Generate Music from the Trained Model\n",
    "def generate_music(model, start_sequence, int_to_note, num_notes=100):\n",
    "    model.eval()\n",
    "    generated_notes = start_sequence.tolist()  # Convert input to list if it's a tensor\n",
    "    \n",
    "    # Initialize hidden state for batch size 1\n",
    "    hidden = model.init_hidden(batch_size=1)  # Ensure batch size matches the input\n",
    "\n",
    "    # Prepare the input sequence by adding a batch dimension\n",
    "    input_sequence = torch.tensor(start_sequence, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "\n",
    "    for _ in range(num_notes):\n",
    "        output, hidden = model(input_sequence, hidden)  # Forward pass through the model\n",
    "        probabilities = nn.functional.softmax(output[:, -1, :], dim=-1)\n",
    "        predicted_note = torch.multinomial(probabilities, num_samples=1).item()\n",
    "        \n",
    "        generated_notes.append(predicted_note)\n",
    "        \n",
    "        # Prepare the next input sequence by appending the predicted note\n",
    "        predicted_note_tensor = torch.tensor([[predicted_note]], dtype=torch.float32).to(device)\n",
    "        input_sequence = torch.cat([input_sequence[:, 1:, :], predicted_note_tensor.unsqueeze(0)], dim=1)\n",
    "    \n",
    "    # Convert generated notes back to readable note names\n",
    "    return [int_to_note[note] for note in generated_notes]\n",
    "\n",
    "\n",
    "\n",
    "# Main Script\n",
    "# Example MIDI-like data\n",
    "midi_data = [\"C4\", \"D4\", \"E4\", \"F4\", \"G4\", \"A4\", \"B4\", \"C5\"] * 100  # Simplified example\n",
    "\n",
    "# Preprocess the data\n",
    "sequence_length = 50\n",
    "input_sequences, output_sequences, note_to_int, int_to_note = preprocess_data(midi_data, sequence_length)\n",
    "\n",
    "# Prepare dataset and dataloader\n",
    "dataset = MusicDataset(input_sequences, output_sequences)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Model Configuration\n",
    "input_size = 1\n",
    "hidden_size = 128\n",
    "output_size = len(note_to_int)\n",
    "num_layers = 2\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "model = MusicRNN(input_size, hidden_size, output_size, num_layers).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training Loop\n",
    "print(\"Starting Training...\")\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "\n",
    "    for inputs, targets in dataloader:\n",
    "        batch_size = inputs.size(0)\n",
    "        hidden = model.init_hidden(batch_size)\n",
    "\n",
    "        inputs = inputs.unsqueeze(-1).to(device)  # Add feature dimension\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs, hidden = model(inputs, hidden)\n",
    "        loss = criterion(outputs[:, -1, :], targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss / len(dataloader):.4f}\")\n",
    "\n",
    "print(\"Training Complete.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0041abae-79fe-4d9f-8b2e-c06b75e72362",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_music(model, start_sequence, int_to_note, num_notes=100):\n",
    "    model.eval()\n",
    "    generated_notes = start_sequence.tolist()  # Convert input sequence to a list\n",
    "\n",
    "    # Initialize hidden state for batch size 1\n",
    "    batch_size = 1\n",
    "    hidden = model.init_hidden(batch_size)  # Ensure hidden state matches batch size\n",
    "\n",
    "    # Ensure input sequence is 3D (batch_size=1, sequence_length, input_size=1)\n",
    "    input_sequence = torch.tensor(start_sequence, dtype=torch.float32).unsqueeze(0).unsqueeze(-1).to(device)\n",
    "\n",
    "    for _ in range(num_notes):\n",
    "        # Forward pass through the model\n",
    "        output, hidden = model(input_sequence, hidden)\n",
    "\n",
    "        # Get probabilities for the next note\n",
    "        probabilities = nn.functional.softmax(output[:, -1, :], dim=-1)\n",
    "        predicted_note = torch.multinomial(probabilities, num_samples=1).item()\n",
    "\n",
    "        # Append the predicted note to the sequence\n",
    "        predicted_note_tensor = torch.tensor([[predicted_note]], dtype=torch.float32).to(device).unsqueeze(-1)\n",
    "        input_sequence = torch.cat([input_sequence[:, 1:, :], predicted_note_tensor], dim=1)\n",
    "\n",
    "    # Convert generated notes to readable format\n",
    "    return [int_to_note[note] for note in generated_notes]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f0d3ffe3-4277-4d43-b0b2-70d92b0c10a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Music...\n",
      "Generated Notes: ['C4', 'D4', 'E4', 'F4', 'G4', 'A4', 'B4', 'C5', 'C4', 'D4', 'E4', 'F4', 'G4', 'A4', 'B4', 'C5', 'C4', 'D4', 'E4', 'F4', 'G4', 'A4', 'B4', 'C5', 'C4', 'D4', 'E4', 'F4', 'G4', 'A4', 'B4', 'C5', 'C4', 'D4', 'E4', 'F4', 'G4', 'A4', 'B4', 'C5', 'C4', 'D4', 'E4', 'F4', 'G4', 'A4', 'B4', 'C5', 'C4', 'D4']\n"
     ]
    }
   ],
   "source": [
    "# Generate music\n",
    "print(\"Generating Music...\")\n",
    "start_sequence = input_sequences[0]  # Use the first sequence as the seed\n",
    "generated_notes = generate_music(model, start_sequence, int_to_note, num_notes=100)\n",
    "print(\"Generated Notes:\", generated_notes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a39339f7-c32d-4801-a372-fe49eb12a598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MIDI file saved as generated_music.mid\n"
     ]
    }
   ],
   "source": [
    "from midiutil import MIDIFile\n",
    "\n",
    "def create_midi_file(notes, filename=\"generated_music.mid\", duration=1, tempo=120):\n",
    "    \"\"\"\n",
    "    Creates a MIDI file from a list of notes.\n",
    "    :param notes: List of note names or MIDI pitches (integers).\n",
    "    :param filename: Output file name for the MIDI file.\n",
    "    :param duration: Duration of each note in beats.\n",
    "    :param tempo: Tempo in beats per minute.\n",
    "    \"\"\"\n",
    "    # MIDI file with one track\n",
    "    midi = MIDIFile(1)\n",
    "    track = 0\n",
    "    time = 0  # Start at the beginning\n",
    "    midi.addTempo(track, time, tempo)  # Set tempo\n",
    "\n",
    "    for note in notes:\n",
    "        # Convert note name to MIDI pitch if needed (e.g., \"C4\" -> 60)\n",
    "        if isinstance(note, str):\n",
    "            pitch = note_to_midi_pitch(note)  # Convert note name to MIDI pitch\n",
    "        else:\n",
    "            pitch = note  # If already a MIDI pitch\n",
    "        \n",
    "        # Add the note\n",
    "        midi.addNote(track, channel=0, pitch=pitch, time=time, duration=duration, volume=100)\n",
    "        time += duration  # Move to the next note time\n",
    "\n",
    "    # Write the MIDI file\n",
    "    with open(filename, \"wb\") as output_file:\n",
    "        midi.writeFile(output_file)\n",
    "    print(f\"MIDI file saved as {filename}\")\n",
    "\n",
    "def note_to_midi_pitch(note):\n",
    "    \"\"\"\n",
    "    Converts a note name to a MIDI pitch.\n",
    "    :param note: Note name (e.g., \"C4\").\n",
    "    :return: MIDI pitch (e.g., 60 for \"C4\").\n",
    "    \"\"\"\n",
    "    note_map = {\"C\": 0, \"D\": 2, \"E\": 4, \"F\": 5, \"G\": 7, \"A\": 9, \"B\": 11}\n",
    "    octave = int(note[-1])  # Extract octave\n",
    "    key = note[:-1]  # Extract note name\n",
    "    semitone = note_map[key[0]]\n",
    "    if len(key) > 1:  # Handle sharps (#) and flats (b)\n",
    "        if key[1] == \"#\":\n",
    "            semitone += 1\n",
    "        elif key[1] == \"b\":\n",
    "            semitone -= 1\n",
    "    return 12 * (octave + 1) + semitone\n",
    "\n",
    "# Generate MIDI file from notes\n",
    "create_midi_file(generated_notes, filename=\"generated_music.mid\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3f4f65ea-421c-4b39-a229-2ee8e7432bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically play the MIDI file\n",
    "import os\n",
    "import platform\n",
    "\n",
    "def play_midi_file(filename):\n",
    "    if platform.system() == \"Darwin\":  # macOS\n",
    "        os.system(f\"open {filename}\")\n",
    "    elif platform.system() == \"Windows\":  # Windows\n",
    "        os.system(f\"start {filename}\")\n",
    "    elif platform.system() == \"Linux\":  # Linux\n",
    "        os.system(f\"xdg-open {filename}\")\n",
    "    else:\n",
    "        print(\"Unable to automatically play the MIDI file. Open it manually.\")\n",
    "\n",
    "play_midi_file(\"generated_music.mid\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828b7bf7-86ba-4f21-b277-0f20f42d1a8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
